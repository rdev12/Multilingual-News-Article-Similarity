{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uKm4iO2hekQs"},"outputs":[],"source":["# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","# !apt-get update -qq 2>&1 > /dev/null\n","# !apt-get -y install -qq google-drive-ocamlfuse fuse\n","# from google.colab import auth\n","# auth.authenticate_user()\n","# from oauth2client.client import GoogleCredentials\n","# creds = GoogleCredentials.get_application_default()\n","# import getpass\n","# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","# vcode = getpass.getpass()\n","# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","# %cd /content\n","# !mkdir drive\n","# %cd drive\n","# !mkdir MyDrive\n","# %cd ..\n","# %cd ..\n","# !google-drive-ocamlfuse /content/drive/MyDrive"]},{"cell_type":"markdown","metadata":{"id":"4y9_FOhlCCD3"},"source":["------\n","### Library setup and mounting g-drive \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwfWcjWH6eTI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQQAeojPRzle"},"outputs":[],"source":["!pip install --quiet transformers\n","!pip install --quiet fasttext\n","# !pip install --quiet tensorflow==1.15.0\n","# restart after running the above line"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XuSnKgpYkOb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","\n","from tqdm.notebook import tqdm\n","import torch\n","import fasttext\n","from sklearn.metrics.pairwise import cosine_similarity\n","from collections import defaultdict\n","import nltk\n","import json\n","from nltk.tokenize import sent_tokenize\n","from nltk.corpus import stopwords\n","import networkx as nx\n","from sklearn import preprocessing\n","import transformers\n","\n","nltk.download(\"punkt\") \n","nltk.download('stopwords')\n","tqdm.pandas()\n","\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","RANDOM_SEED = 42\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhuETOSmDLPz"},"outputs":[],"source":["# # download pretrained EMLO embeddings\n","# %mkdir /content/module\n","# # Download the module, and uncompress it to the destination folder. \n","# !curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC /content/module/\n","# # instantiate pretrained ELMO model\n","# pretrained_elmo = hub.Module(\"/content/module/\", trainable=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UrhaUldyNAK"},"outputs":[],"source":["# initialize fasttext model\n","path_to_embeddings = \"./embeddings/fasttext-embeddings/cc.en.300.bin\"\n","model = fasttext.load_model(path_to_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wLwCxt1ZVjs"},"outputs":[],"source":["# PRE processing utility methods\n","# remove \\n\\n\n","def truncate_newline(text):\n","  # remove double new-line character\n","  text = text.replace(\"\\n\\n\", \" \")\n","  return text\n","\n","# download JSON based file in dictionary format\n","def get_json(path_to_file):\n","  with open(path_to_file, 'r') as openfile:\n","      articles = json.load(openfile)\n","  return articles\n","\n","# save dict -> json\n","def to_json(save_location, file_name, dict_: dict):\n","  try:\n","    with open(f'{save_location}/{file_name}', 'w') as fp:\n","      json.dump(dict_, fp)\n","    return True \n","  except Exception as e:\n","    return e\n","\n","# split text articles into sentences and append to a main dictionary\n","def generate_sentences(data: pd.DataFrame, key='id', value='text'):\n","  article_sentences = dict()\n","  for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n","    id, text = row[key], row[value]\n","    sentences = sent_tokenize(text)\n","    # remove elements equals dot '.'\n","    sentences = [sentence for sentence in sentences if sentence != '.']\n","    article_sentences[id] = sentences\n","  return article_sentences\n","\n","def modifiy_gen_sentences(dict_: dict):\n","  out_dict_ = dict()\n","  for key, value in dict_.items():\n","    out_dict_[key] = {\n","        'text': value\n","    }\n","  return out_dict_\n","\n","# define stopwords\n","stop_words = stopwords.words('english')\n","\n","# function to remove stopwords\n","def remove_stopwords(sen):\n","  sen_new = \" \".join([i for i in sen if i not in stop_words])\n","  return sen_new\n","\n","# preprocessing\n","def preprocess(sentences: list):\n","  # remove punctuations, numbers and special characters\n","  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n","  # make alphabets lowercase\n","  clean_sentences = [s.lower() for s in clean_sentences]\n","  # remove stopwords from the sentences\n","  clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n","  return clean_sentences\n","\n","# generate sentence vectors\n","def get_sentence_vectors(sentences: list, word_embeddings: dict, dim: int=100, fasttext=False, elmo=False):\n","  sentence_vectors = []\n","  for i in sentences:\n","    if fasttext:\n","      if len(i) != 0:\n","        v = sum([model.get_word_vector(w) for w in i.split()])/(len(i.split())+0.001)\n","      else: \n","        v = np.zeros((dim,))\n","    else:\n","      print(\"....\")\n","      if len(i) != 0:\n","        v = sum([word_embeddings.get(w, np.zeros((dim,))) for w in i.split()])/(len(i.split())+0.001)\n","      else:\n","        v = np.zeros((dim,))\n","    # append sentence vectors\n","    sentence_vectors.append(v)\n","  return sentence_vectors\n","\n","# compute the similarity scores between sentences\n","def calc_pairwise_similarity(sentences: list, sentence_vectors: list, dim: int=100):\n","  # similarity matrix\n","  sim_mat = np.zeros([len(sentences), len(sentences)])\n","  # calculate pairwise similarity \"cosine-similarity\"\n","  for i in range(len(sentences)):\n","    for j in range(len(sentences)):\n","      if i != j:\n","        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,dim), sentence_vectors[j].reshape(1,dim))[0,0]\n","  return sim_mat\n","\n","# split scores and ranked texts\n","def process_ranked_text(ranked_sentences: list):\n","  scores, sorted_texts = list(zip(*ranked_sentences))\n","  return list(scores), list(sorted_texts)\n","\n","# processing method for process_batch\n","def process_instance(sentences: list, dim: int=100, fasttext=False, elmo=False):\n","  # preprocess sentences\n","  clean_sentences = preprocess(sentences)\n","  # retrieve corresponding sentence vectors\n","  sentence_vectors = get_sentence_vectors(clean_sentences, word_embeddings, dim, fasttext, elmo)\n","  # similarity matrix\n","  sim_mat = calc_pairwise_similarity(sentences, sentence_vectors, dim)\n","  # create graph and apply pagerank algorithm\n","  nx_graph = nx.from_numpy_array(sim_mat)\n","  scores = nx.pagerank(nx_graph, max_iter=600)\n","  # sort the scored sentences\n","  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n","  return ranked_sentences\n","\n","# processing method for process_batch 'semeval'\n","def process_instance_semeval(sentences: list, dim: int=100, fasttext=False, elmo=False):\n","  # preprocess sentences\n","  clean_sentences = preprocess(sentences)\n","  # retrieve corresponding sentence vectors\n","  sentence_vectors = get_sentence_vectors(clean_sentences, word_embeddings, dim, fasttext, elmo)\n","  # similarity matrix\n","  sim_mat = calc_pairwise_similarity(sentences, sentence_vectors, dim)\n","  # create graph and apply pagerank algorithm\n","  nx_graph = nx.from_numpy_array(sim_mat)\n","  scores = nx.pagerank(nx_graph, max_iter=600)\n","  # sort the scored sentences\n","  ranked_sentences = [(scores[idx], sentence) for idx, sentence in enumerate(sentences)]\n","  return ranked_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_RNV_kYKEyr"},"outputs":[],"source":["# POST processing utility methods\n","# split sentences using the partition key\n","def split_content(List: list, key: int):\n","  return List[:key], List[key:]\n","\n","# sort sentences w.r.t to scores\n","def sort_sentences(sentences_: list, scores_: list):\n","  return sorted(((scores_[idx], sentence) for idx, sentence in enumerate(sentences_)), reverse=True)\n","\n","# post batch processing\n","def post_batch_process(ranked_sentence_dict: dict, merge_dict: dict):\n","  # final dict to return\n","  final_partitioned_dict = dict()\n","  # --DEBUG--\n","  # faulty indices \n","  faulty_indices = []\n","  for key, value in tqdm(ranked_sentence_dict.items(), total=len(ranked_sentence_dict.keys())):\n","    try:\n","      # retrieve key from main dictionary\n","      partition_key = merge_dict[key]['partition_no']\n","      # split scores, sentences & index\n","      scores_A, scores_B = split_content(value['score'], partition_key)\n","      text_A, text_B = split_content(value['text'], partition_key)\n","      index_A, index_B = key.split('_')\n","      # sort sentences \n","      scores_a, text_a = process_ranked_text(sort_sentences(text_A, scores_A))\n","      scores_b, text_b = process_ranked_text(sort_sentences(text_B, scores_B)) \n","      # attach instances to the dict\n","      final_partitioned_dict[index_A] = dict(\n","          score = scores_a,\n","          text = text_a,\n","      )\n","      final_partitioned_dict[index_B] = dict(\n","          score = scores_b,\n","          text = text_b,\n","      )\n","    except Exception as e:\n","      faulty_indices.append((key))\n","  return final_partitioned_dict, faulty_indices\n","\n","# merge 2 dictionaries of varied length\n","def merge_dictionaries(a, b):\n","   return {**a, **b}\n","\n","# get pairs\n","def get_pair_indices(data: pd.DataFrame, sep='_'):\n","  index_pair = data.progress_apply(\n","      lambda row: row['pair_id'].split(sep),\n","      axis=1\n","  )\n","  id_1, id_2 = list(zip(*index_pair.to_list()))\n","  return id_1, id_2\n","\n","# generate pair dict\n","def generate_pair_dict(target_dict: dict, index_a: list, index_b: list):\n","  proposed_dict = dict()\n","  # debug \n","  faulty_pairs = []\n","  # -----\n","  for serial_nos, (id_1, id_2) in tqdm(enumerate(zip(index_a, index_b)), total=len(index_a)):\n","    # note: not attaching the scores\n","    try:\n","      proposed_dict[f'sr_{serial_nos}'] = dict(\n","          index_a=id_1,\n","          index_b=id_2,\n","          text_a=target_dict[id_1]['text'],\n","          text_b=target_dict[id_2]['text'],\n","      )\n","    except Exception as e:\n","      faulty_pairs.append(f\"{id_1}_{id_2}\")\n","  return proposed_dict, faulty_pairs\n","\n","# apply sorting to ranked data to get top-k most preferable sentences\n","def reorder_ranked_data(ranked_dict: dict):\n","  reordered_dict = dict()\n","  for key, value in tqdm(ranked_dict.items(), total=len(ranked_dict.keys())):\n","    scores = value['score']\n","    sentences = value['text']\n","    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n","    sorted_scores, sorted_ranked_texts = process_ranked_text(ranked_sentences)\n","    reordered_dict[key] = {\n","        'score': sorted_scores,\n","        'text': sorted_ranked_texts,\n","    }\n","  return reordered_dict\n","\n","# extract top_k pairs\n","def extract_top(pair: dict, tokenizer: transformers.AutoTokenizer, k=12, MAX_TOKENS=512):\n","  # create temp. dictionary\n","    result = {\n","        pair[\"index_a\"]: [],\n","        pair[\"index_b\"]: []\n","    }\n","    token_count = 0\n","    for i in range(0, k):\n","      # DOCUMENT A\n","      if (i < len(pair['text_a'])):\n","        # (sentence, score)\n","        sentence_a = pair['text_a'][i]\n","        # get tokens -> sentence\n","        tokens_a = tokenizer.encode(\n","            sentence_a,\n","            max_length=512,\n","            truncation=True,\n","        )\n","        # check for total size of overall text i.e. approved(text_a) + <sep> + approved(text_b)\n","        add_len_a = min(len(tokens_a), max(0, MAX_TOKENS - token_count))\n","        if (add_len_a == 0):\n","          break\n","        # if total_len < MAX_TOKEN_COUNT :-\n","        result[pair['index_a']].append(tokenizer.decode(tokens_a[0:add_len_a], skip_special_tokens=True))\n","        token_count += add_len_a\n","      # DOCUMENT B\n","      if (i < len(pair['text_b'])):\n","        # (sentence, score)\n","        sentence_b = pair['text_b'][i]\n","        # get tokens -> sentence\n","        tokens_b = tokenizer.encode(\n","              sentence_b,\n","              max_length=512,\n","              truncation=True,\n","        )\n","        # check for total size of overall text i.e. approved(text_a) + <sep> + approved(text_b)\n","        add_len_b = min(len(tokens_b), max(0, MAX_TOKENS - token_count))\n","        if (add_len_b == 0):\n","          break\n","        # if total_len < MAX_TOKEN_COUNT :-\n","        result[pair['index_b']].append(tokenizer.decode(tokens_b[0:add_len_b], skip_special_tokens=True))\n","        token_count += add_len_b\n","\n","    return result\n","\n","# --------------------------------------------------------------------------------\n","# Extract top k sentence from ranked hyperpartisan dataset\n","def extract_top_hyp(text: list, tokenizer: transformers.AutoTokenizer, k=12, MAX_TOKENS=512):\n","    token_count = 0\n","    result = []\n","    for i in range(0, k):\n","      # DOCUMENT A\n","      if (i < len(text)):\n","        # (sentence, score)\n","        sentence = text[i]\n","        # get tokens -> sentence\n","        tokens = tokenizer.encode(\n","            sentence,\n","            max_length=512,\n","            truncation=True,\n","        )\n","\n","        add_len_a = min(len(tokens), max(0, MAX_TOKENS - token_count))\n","        if (add_len_a == 0):\n","          break\n","        # if total_len < MAX_TOKEN_COUNT :-\n","        result.append(tokenizer.decode(tokens[0:add_len_a], skip_special_tokens=True))\n","        token_count += add_len_a\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f03rZXWlaat"},"outputs":[],"source":["# glove methods\n","# apply textrank on single instance\n","def process_single(sentences: list, word_embeddings: dict, dim: int=100, fasttext=False, elmo=False):\n","  # preprocess sentences\n","  clean_sentences = preprocess(sentences)\n","  # retrieve corresponding sentence vectors\n","  sentence_vectors = get_sentence_vectors(clean_sentences, word_embeddings, dim, fasttext, elmo)\n","  # similarity matrix\n","  sim_mat = calc_pairwise_similarity(sentences, sentence_vectors, dim)\n","  # create graph and apply pagerank algorithm\n","  nx_graph = nx.from_numpy_array(sim_mat)\n","  scores = nx.pagerank(nx_graph)\n","  # sort the scored sentences\n","  # ranked_sentences = [(scores[idx], sentence) for idx, sentence in enumerate(sentences)]\n","  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n","  return ranked_sentences\n","\n","# process entire data\n","def process_batch(article_sentences: dict, word_embeddings, dim=100, file_name='preprocess.json', flag=False, fasttext=False, elmo=False):\n","  count = 0\n","  overall_dict = dict()\n","  faulty_ids = []\n","  for key, value in tqdm(article_sentences.items(), total=len(article_sentences.keys())):\n","    try:\n","      if flag:\n","        sorted_sentences = process_instance(value['text'], dim, fasttext, elmo)\n","      else:\n","        sorted_sentences = process_instance_semeval(value['text'], dim, fasttext, elmo)\n","      scores, ranked_texts = process_ranked_text(sorted_sentences)\n","      count += 1\n","    except Exception as e:\n","      print(e)\n","      faulty_ids.append(key)\n","      continue\n","    overall_dict[key] = dict(\n","        score=scores,\n","        text=ranked_texts\n","    )\n","    # save after every 1000 counts\n","    if count%500 == 0:\n","      to_json(path_to_dir, file_name, overall_dict)\n","\n","  return overall_dict, faulty_ids\n","\n","# USE BERT IF RESULTS ARE NOT BETTER."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAoj7TvRXHCk"},"outputs":[],"source":["# download and unzip glove embeddings\n","# !wget http://nlp.stanford.edu/data/glove.6B.zip\n","# !unzip glove*.zip\n","\n","# Extract word vectors \n","word_embeddings = {}\n","path_to_glove_embeddings = \"./embeddings/glove.6B.100d.txt\"\n","f = open(path_to_glove_embeddings, encoding='utf-8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    word_embeddings[word] = coefs\n","f.close()\n","len(word_embeddings)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnfedWfxTZ5v"},"outputs":[],"source":["# initialize huggingface tokenizer\n","tokenizer = transformers.AutoTokenizer.from_pretrained('xlm-roberta-base')"]},{"cell_type":"markdown","metadata":{"id":"1VM-FFtpuTEV"},"source":["-----\n","#### PROCESSING HYPERPARTISAN DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yqim6wYJuaJD"},"outputs":[],"source":["# path_to_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/part_0.csv'\n","# data_df = pd.read_csv(path_to_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttNJ1Tqcu0UO"},"outputs":[],"source":["# # # retrieve sentences w.r.t ids\n","# article_sentences = generate_sentences(data_df, key='idx', value='text')\n","# article_sentences = modifiy_gen_sentences(article_sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTOwBNDyy4yH"},"outputs":[],"source":["# # splitted dataset into 3 equal parts and ran parallel notebooks for faster execution. \n","# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data\"\n","# processed_dict, faulty_unique_ids = process_batch(article_sentences, word_embeddings, dim=100, file_name='preprocessed_data.json')\n","# # convert keys to str\n","# processed_dict = {str(key): value for key, value in processed_dict.items()}\n","# # save dictionary to json \n","# to_json(path_to_dir, 'preproc_part2.json', processed_dict)"]},{"cell_type":"markdown","metadata":{"id":"mqaDOyjsWs32"},"source":["-----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ilh3jOAzWt1G"},"outputs":[],"source":["# # join and concat ranked data using a topk extraction\n","# ranked_data = [{'idx': key, 'text': \" \".join(extract_top_hyp(value['text'], tokenizer))} for key, value in tqdm(processed_dict.items(), total=len(processed_dict.keys()))]\n","# processed_df = pd.DataFrame(ranked_data, columns=['idx', 'text'])       # processed & ranked data\n","# target_df = data_df[['idx', 'title', 'bias', 'hyperpartisan']]          # target dataframe\n","# # change dtype\n","# target_df['idx'] = target_df['idx'].astype('str')     \n","# processed_df = pd.merge(processed_df, target_df, on='idx')              # merged dataframe\n","# # save processed dataframe\n","# # processed_data__.to_csv(f'{path_to_dir}/final_data.csv', index=False) # üíÄ"]},{"cell_type":"markdown","metadata":{"id":"Nbd46kGSo0xF"},"source":["`Data files are merged and is saved as file_data.csv`\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d51GDeleivYq"},"source":["------\n","#### PROCESS TEST DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcniCNTci1ms"},"outputs":[],"source":["# # main semeval data\n","# path_to_semeval_data = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/test_v1.csv\"\n","# main_df = pd.read_csv(path_to_semeval_data)\n","# # combination of lang1 and lang2 pairs required\n","# combination_df = pd.DataFrame({\n","#     'url1_lang': main_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})['url1_lang'].tolist(),\n","#     'url2_lang': main_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})['url2_lang'].tolist()\n","# })\n","# # resultant dataframe\n","# target_df = pd.merge(main_df, combination_df)\n","# # check for the retrieved data\n","# target_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcsDvJfvkYWq"},"outputs":[],"source":["# # non-english data\n","# title_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/title/merged_test_title.csv')\n","# desc_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/extra_text/merged_test_extra_text.csv')\n","# text_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/merged_test_text_v1.csv')\n","# # english data\n","# title_en_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/english_test/title_en.csv')\n","# desc_en_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/english_test/extra_text_en.csv')\n","# text_en_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/english_test/doc_en.csv')\n","# # non-english data\n","# print(title_df.shape, desc_df.shape, text_df.shape)\n","# # english data\n","# print(title_en_df.shape, desc_en_df.shape, text_en_df.shape)\n","# # rename data\n","# title_df.rename(columns={'text': \"title\"}, inplace=True, errors=\"raise\")\n","# desc_df.rename(columns={'text': 'extra_text'}, inplace=True, errors=\"raise\")\n","# # # concat files\n","# main_title_df = pd.concat([title_df, title_en_df])\n","# main_desc_df = pd.concat([desc_df, desc_en_df])\n","# main_text_df = pd.concat([text_df, text_en_df])\n","# print(main_title_df.shape, main_desc_df.shape, main_text_df.shape)\n","# # merge DataFrames\n","# main_df = pd.merge(main_title_df, main_desc_df, on='id', how='outer')\n","# main_df = pd.merge(main_df, main_text_df, on='id', how='outer')   \n","# main_df.fillna('', inplace=True)\n","# # save file\n","# main_df.to_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/translated_test_v1.csv', index=False)\n","# main_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4P51pbukw9Mb"},"outputs":[],"source":["# # get index pairs\n","# index_a, index_b = get_pair_indices(target_df)\n","\n","# # path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data/merged_translation_data.csv'\n","# path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/translated_test_v1.csv'\n","# data_df = pd.read_csv(path_to_merged_file)\n","# data_df['id'] = data_df['id'].astype(str)\n","\n","# data_df.fillna(' ', inplace=True)\n","\n","# # merge title, desc and text\n","# data_df['new_text'] = data_df.progress_apply(\n","#     lambda row: row['title'] + '.' + row['extra_text'] + '.' + row['text'],\n","#     axis = 1\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB7Uc09kGoUI"},"outputs":[],"source":["# # get index pairs\n","# index_a, index_b = get_pair_indices(target_df)\n","\n","# # path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data/merged_translation_data.csv'\n","# path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data_test/translated_test_v1.csv'\n","# data_df = pd.read_csv(path_to_merged_file)\n","# data_df['id'] = data_df['id'].astype(str)\n","\n","# data_df.fillna(' ', inplace=True)\n","\n","# # merge title, desc and text\n","# data_df['new_text'] = data_df.progress_apply(\n","#     lambda row: row['title'] + '.' + row['extra_text'] + '.' + row['text'],\n","#     axis = 1\n","# )\n","\n","# data_df.drop(['title', 'extra_text', 'text'], axis=1, inplace=True)\n","# data_df.rename(columns={\n","#     'new_text': 'text'\n","# }, inplace=True, errors=\"raise\")\n","\n","# # retrieve sentences w.r.t ids\n","# article_sentences = generate_sentences(data_df)\n","\n","# # # get final dict for textrank processing\n","# Final_dict = dict()\n","# faulty_ids = []\n","# for serial_nos, (id_1, id_2) in tqdm(enumerate(zip(index_a, index_b)), total=len(index_a)):\n","#   try:\n","#     # retrieve text respective to the ids \n","#     text_a = article_sentences[id_1]\n","#     text_b = article_sentences[id_2]\n","#     # assign the unique id\n","#     unique_id = f'{id_1}_{id_2}'\n","#     # extend the list of sentence from text_a and text_b\n","#     text_a.extend(text_b)\n","#     # attach to the Final_dict\n","#     Final_dict[unique_id] = dict(\n","#         text = text_a,\n","#         partition_no = len(text_a)-len(text_b)\n","#     )\n","#   except Exception as e:\n","#     faulty_ids.append((id_1, id_2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_PVC_PiN8s-"},"outputs":[],"source":["# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv\"\n","# processed_dict, faulty_unique_ids = process_batch(Final_dict, word_embeddings, dim=300, fasttext=True, file_name=\"preprocess_test_v2.json\")\n","# # Save file as JSON.\n","# to_json(save_location=path_to_dir, file_name=\"ranked_merged_test_dict_v2_elmo.json\", dict_=processed_dict) # processed dict\n","# to_json(save_location=path_to_dir, file_name=\"merged_&_pos_test_data_v2_elmo.json\", dict_=Final_dict) # Final dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nsa2QrbIS0Co"},"outputs":[],"source":["# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv\"\n","# # load final and procesed dict\n","# ranked_dict = get_json(f\"{path_to_dir}/ranked_merged_test_dict_v1.json\")\n","# final_dict = get_json(f\"{path_to_dir}/merged_&_pos_test_data_v1.json\")\n","\n","# # apply post ranking processes\n","# ranked_data, faulty_indices = post_batch_process(ranked_dict, final_dict)\n","# # apply sorting to each individual articles \n","# ranked_data = reorder_ranked_data(ranked_data)\n","\n","# # save ranked_data\n","# to_json(path_to_dir, 'ranked_semeval_test_data_v2.json', ranked_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-EsEStqcsAB"},"outputs":[],"source":["# # ranked data\n","# path_to_ranked_data = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv/ranked_semeval_test_data_v2.json'\n","# ranked_dict = get_json(path_to_ranked_data)\n","\n","# # main semeval data\n","# path_to_semeval_data = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/test_v1.csv\"\n","# main_df = pd.read_csv(path_to_semeval_data)\n","# # combination of lang1 and lang2 pairs required\n","# combination_df = pd.DataFrame({\n","#     'url1_lang': main_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})['url1_lang'].tolist(),\n","#     'url2_lang': main_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})['url2_lang'].tolist()\n","# })\n","# # resultant dataframe\n","# target_df = pd.merge(main_df, combination_df)\n","# # check for the retrieved data\n","# target_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHr3PA_Sc55V"},"outputs":[],"source":["# # generate paired data\n","# index_a, index_b = get_pair_indices(target_df)\n","# final_dict, faulty_pairs = generate_pair_dict(ranked_dict, index_a, index_b)\n","\n","# # apply top k extraction method to entire data.\n","# stratified_final_dict = defaultdict(list)\n","# for serial_nos in tqdm(final_dict.keys(), total=len(final_dict.keys())):\n","#   # keys = range(sr_0, sr_1, ...., sr_N)\n","#   data = final_dict[serial_nos]\n","#   # extracting top_k sentences (sorted)\n","#   output = extract_top(data, tokenizer, k=12, MAX_TOKENS=512)\n","\n","#   stratified_final_dict['index_a'].append(data['index_a'])\n","#   stratified_final_dict['index_b'].append(data['index_b'])\n","#   stratified_final_dict['text_a'].append(\" \".join(output[data['index_a']]))\n","#   stratified_final_dict['text_b'].append(\" \".join(output[data['index_b']])) \n","  \n","# # create final dataframe\n","# finalised_df = pd.DataFrame(stratified_final_dict)\n","# finalised_df['pair_id'] = finalised_df.progress_apply(\n","#     lambda row: f\"{row['index_a']}_{row['index_b']}\",\n","#     axis=1\n","# )\n","# finalised_df.drop(['index_a', 'index_b'], axis=1, inplace=True)\n","# finalised_df = finalised_df[['pair_id', 'text_a', 'text_b']]\n","# print(f\"finalised_df: {finalised_df.shape}\")\n","\n","# # merge target and finalised dataframes based on \"pair_id\".\n","# main_df = pd.merge(target_df, finalised_df, on='pair_id', how='outer') \n","# main_df.dropna(inplace=True)\n","# print(f\"main_df: {main_df.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kc8yXBcAlPxG"},"outputs":[],"source":["# # remove duplicates\n","# main_df = main_df[~main_df.duplicated()]\n","# save_loc = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv/final_test_v5.csv\"\n","# main_df.to_csv(save_loc, index=False)"]},{"cell_type":"markdown","metadata":{"id":"97gOnbppyWLl"},"source":["------\n","#### INITIAL PROCESSING\n","* Merge all the sentences in both text artilces ‚úîÔ∏è\n","* Also try implementing the simialrity score used over [here](https://assistant.raxter.io/projects/semeval-2022-multilingual-document-similarity1640024433930/literatures/2101064231640675351177) ‚ùå"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtmKX-JgFHFE"},"outputs":[],"source":["# # main semeval data\n","# path_to_semeval_data = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/combined_train_v1.csv\"\n","# main_df = pd.read_csv(path_to_semeval_data)\n","# # combination of lang1 and lang2 pairs required\n","# combination_df = pd.DataFrame({\n","#     'url1_lang': ['de', 'de', 'en', 'es', 'tr', 'pl', 'fr', 'ar'],\n","#     'url2_lang': ['de', 'en', 'en', 'es', 'tr', 'pl', 'fr', 'ar']\n","# })\n","# # resultant dataframe\n","# target_df = pd.merge(main_df, combination_df)\n","# # check for the retrieved data\n","# target_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FG-AjkznJL6"},"outputs":[],"source":["Z# # get index pairs\n","# index_a, index_b = get_pair_indices(target_df)\n","\n","# # path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data/merged_translation_data.csv'\n","# path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translated_train_processed_v2.csv'\n","# data_df = pd.read_csv(path_to_merged_file)\n","# data_df['id'] = data_df['id'].astype(str)\n","\n","# # retrieve sentences w.r.t ids\n","# article_sentences = generate_sentences(data_df)\n","\n","# # # get final dict for textrank processing\n","# Final_dict = dict()\n","# faulty_ids = []\n","# for serial_nos, (id_1, id_2) in tqdm(enumerate(zip(index_a, index_b)), total=len(index_a)):\n","#   try:\n","#     # retrieve text respective to the ids \n","#     text_a = article_sentences[id_1]\n","#     text_b = article_sentences[id_2]\n","#     # assign the unique id\n","#     unique_id = f'{id_1}_{id_2}'\n","#     # extend the list of sentence from text_a and text_b\n","#     text_a.extend(text_b)\n","#     # attach to the Final_dict\n","#     Final_dict[unique_id] = dict(\n","#         text = text_a,\n","#         partition_no = len(text_a)-len(text_b)\n","#     )\n","#   except Exception as e:\n","#     faulty_ids.append((id_1, id_2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tm-wdJLeboGI"},"outputs":[],"source":["# # don not run this üíÄ‚ùå \n","# data_df = pd.read_csv(\"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translated_train_v2.csv\")\n","# # fill nan with ''\n","# data_df.fillna('', inplace=True)\n","# # append '.' with title and description.\n","# data_df['title'] = data_df.progress_apply(\n","#     lambda row: row['title'] if row['title'].endswith('.') or row['title'].endswith('?') else row['title']+\".\",\n","#     axis=1\n","# )\n","# data_df['desc'] = data_df.progress_apply(\n","#     lambda row: row['desc'] if row['desc'].endswith('.') or row['desc'].endswith('?') else row['desc']+\".\",\n","#     axis=1\n","# )\n","# # merge title + desc + text\n","# data_df['merged_text'] = data_df.progress_apply(\n","#     lambda row: row['title'] +\" \"+ row['desc'] +\" \"+ row['text'],\n","#     axis=1\n","# )\n","# data_df['merged_text'] = data_df.progress_apply(\n","#     lambda row: row['merged_text'].strip(),\n","#     axis=1\n","# )\n","# # drop unnecessary columns and rename merged_text\n","# data_df.drop(['title', 'desc', 'text'], axis=1, inplace=True)\n","# data_df.rename(columns={\n","#     'merged_text': 'text'\n","# }, inplace=True, errors='raise')\n","# # calculate word count\n","# data_df['word_count'] = data_df.progress_apply(\n","#     lambda row: len(row['text'].split(' ')),\n","#     axis=1\n","# ) \n","# ####################### EXTRA PROCESSING BASED ON CONTEXT ######################\n","# # selecting rows with word_count > 30 (might contain some anamolies)\n","# factored_data = data_df[data_df['word_count'] > 30]\n","# # drop the analysed index \n","# # rem_index = [3968, 4036, 4075, 4106, 4342, 4503, 7494, 7590, 4579,]\n","# rem_index = [1483881256, 1484028040, 1548289807, 1555147462, 1548275840, 1484144810, 1484084344, 1510785328, 1483931224, 1484312115, 1484026894, 1584880806, 1484084350, 1484285197, 1484084356, 1484137472, 1484285188, 1484137480, 1484109196, 1484299489, 1484339239]\n","# final_df = factored_data.drop(factored_data[factored_data['id'].isin(rem_index)].index.tolist())\n","# final_df.drop(['word_count'], axis=1, inplace=True)\n","# final_df.reset_index(drop=True, inplace=True)\n","# # save file\n","# # data_df.to_csv(\"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translated_train_processed.csv\", index=False)\n","# final_df.to_csv(\"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translated_train_processed_v2.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSTbonhREUQU"},"outputs":[],"source":["# # get index pairs\n","# index_a, index_b = get_pair_indices(target_df)\n","\n","# # path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translation_data/merged_translation_data.csv'\n","# path_to_merged_file = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/translated_train_processed_v2.csv'\n","# data_df = pd.read_csv(path_to_merged_file)\n","# data_df['id'] = data_df['id'].astype(str)\n","\n","# # retrieve sentences w.r.t ids\n","# article_sentences = generate_sentences(data_df)\n","\n","# # # get final dict for textrank processing\n","# Final_dict = dict()\n","# faulty_ids = []\n","# for serial_nos, (id_1, id_2) in tqdm(enumerate(zip(index_a, index_b)), total=len(index_a)):\n","#   try:\n","#     # retrieve text respective to the ids \n","#     text_a = article_sentences[id_1]\n","#     text_b = article_sentences[id_2]\n","#     # assign the unique id\n","#     unique_id = f'{id_1}_{id_2}'\n","#     # extend the list of sentence from text_a and text_b\n","#     text_a.extend(text_b)\n","#     # attach to the Final_dict\n","#     Final_dict[unique_id] = dict(\n","#         text = text_a,\n","#         partition_no = len(text_a)-len(text_b)\n","#     )\n","#   except Exception as e:\n","#     faulty_ids.append((id_1, id_2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzhGvvdL1whw"},"outputs":[],"source":["# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv\"\n","# processed_dict, faulty_unique_ids = process_batch(Final_dict, word_embeddings, dim=300, fasttext=True, file_name=\"preprocess_v2.json\")\n","# # Save file as JSON.\n","# to_json(save_location=path_to_dir, file_name=\"ranked_merged_dict_v2.json\", dict_=processed_dict) # processed dict\n","# to_json(save_location=path_to_dir, file_name=\"merged_&_pos_data_v2.json\", dict_=Final_dict) # Final dict"]},{"cell_type":"markdown","metadata":{"id":"Jf-kmun98caa"},"source":["---\n","#### POST PROCESSING\n","* split the sentence according to partition index ‚úîÔ∏è\n","* sort the ranked sentences according to derived scores ‚úîÔ∏è\n","* sort and rank individual lists of sentences and scores ‚úîÔ∏è\n","* apply top-K extraction to pairs of index ‚úîÔ∏è"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXK5wOfZ7V5x"},"outputs":[],"source":["# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv\"\n","# # load final and procesed dict\n","# ranked_dict = get_json(f\"{path_to_dir}/ranked_merged_dict_v2.json\")\n","# final_dict = get_json(f\"{path_to_dir}/merged_&_pos_data_v2.json\")\n","\n","# # apply post ranking processes\n","# ranked_data, faulty_indices = post_batch_process(ranked_dict, final_dict)\n","# # apply sorting to each individual articles \n","# ranked_data = reorder_ranked_data(ranked_data)\n","\n","# # save ranked_data\n","# to_json(path_to_dir, 'ranked_semeval_data_v2.json', ranked_data)"]},{"cell_type":"markdown","metadata":{"id":"oAwlqbWZYoaP"},"source":["-----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW1MS6jZPfTv"},"outputs":[],"source":["# # ranked data\n","# path_to_ranked_data = '/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv/ranked_semeval_data_v2.json'\n","# ranked_dict = get_json(path_to_ranked_data)\n","\n","# # main semeval data\n","# path_to_semeval_data = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/combined_train_v1.csv\"\n","# main_df = pd.read_csv(path_to_semeval_data)\n","# # combination of lang1 and lang2 pairs required\n","# combination_df = pd.DataFrame({\n","#     'url1_lang': ['de', 'de', 'en', 'es', 'tr', 'pl', 'ar', 'fr'],\n","#     'url2_lang': ['de', 'en', 'en', 'es', 'tr', 'pl', 'ar', 'fr']\n","# })\n","# # resultant dataframe\n","# target_df = pd.merge(main_df, combination_df)\n","# # check for the retrieved data\n","# target_df.groupby(['url1_lang','url2_lang']).size().reset_index().rename(columns={0:'count'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxnPqgvbZvmS"},"outputs":[],"source":["# # generate paired data\n","# index_a, index_b = get_pair_indices(target_df)\n","# final_dict, faulty_pairs = generate_pair_dict(ranked_dict, index_a, index_b)\n","\n","# # apply top k extraction method to entire data.\n","# stratified_final_dict = defaultdict(list)\n","# for serial_nos in tqdm(final_dict.keys(), total=len(final_dict.keys())):\n","#   # keys = range(sr_0, sr_1, ...., sr_N)\n","#   data = final_dict[serial_nos]\n","#   # extracting top_k sentences (sorted)\n","#   output = extract_top(data, tokenizer, k=12, MAX_TOKENS=512)\n","\n","#   stratified_final_dict['index_a'].append(data['index_a'])\n","#   stratified_final_dict['index_b'].append(data['index_b'])\n","#   stratified_final_dict['text_a'].append(\" \".join(output[data['index_a']]))\n","#   stratified_final_dict['text_b'].append(\" \".join(output[data['index_b']])) \n","  \n","# # create final dataframe\n","# finalised_df = pd.DataFrame(stratified_final_dict)\n","# finalised_df['pair_id'] = finalised_df.progress_apply(\n","#     lambda row: f\"{row['index_a']}_{row['index_b']}\",\n","#     axis=1\n","# )\n","# finalised_df.drop(['index_a', 'index_b'], axis=1, inplace=True)\n","# finalised_df = finalised_df[['pair_id', 'text_a', 'text_b']]\n","# print(f\"finalised_df: {finalised_df.shape}\")\n","\n","# # merge target and finalised dataframes based on \"pair_id\".\n","# main_df = pd.merge(target_df, finalised_df, on='pair_id', how='outer') \n","# main_df.dropna(inplace=True)\n","# print(f\"main_df: {main_df.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oaqGKAXc_Qn"},"outputs":[],"source":["# # remove duplicates\n","# main_df = main_df[~main_df.duplicated()]\n","# save_loc = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv/final_data_v2.csv\"\n","# main_df.to_csv(save_loc, index=False)"]},{"cell_type":"markdown","metadata":{"id":"9acLzPtUGnUk"},"source":["-----\n","#### PREPROCESS SEMEVAL DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fl4soXGtUo9n"},"outputs":[],"source":["import spacy\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.tokenize.toktok import ToktokTokenizer\n","import re\n","import unicodedata\n","\n","nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n","tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words('english')\n","stopword_list.remove('no')\n","stopword_list.remove('not')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNY9NKZaJBXU"},"outputs":[],"source":["main_df = pd.read_csv('../dataset/final_data.csv')\n","main_df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3oebDVSGyJN"},"outputs":[],"source":["# preprocessing methods\n","# replace non definite punctuations\n","def replace_content(text):\n","  replace_dict = {\n","      '‚Äú': '\\'',\n","      '‚Äù': '\\'',\n","      '‚Äò': '\\'',\n","      '‚Äô': '\\'',\n","  }\n","  for key, value in replace_dict.items():\n","    text = text.replace(key, value)\n","  return text\n","\n","# download JSON based file in dictionary format\n","def get_json(path_to_file):\n","  with open(path_to_file, 'r') as openfile:\n","      articles = json.load(openfile)\n","  return articles\n","\n","# get contraction file in JSON format\n","CONTRACTION_MAP = get_json('../dataset/contractions.json')\n","\n","# replace pronounciation chars with normalized alphabets\n","def remove_accented_chars(text):\n","  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","  return text\n","\n","# remove content withing brackets\n","def remove_text_within_brackets(text):\n","  text = re.sub('\\[.*?\\]', '', text)\n","  text = re.sub('\\(.*?\\)', '', text)\n","  return text\n","\n","# remove URLs\n","def remove_hyperlinks(text):\n","  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","  return text\n","\n","# expand contraction\n","def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n","  contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n","                                    flags=re.IGNORECASE|re.DOTALL)\n","  def expand_match(contraction):\n","      match = contraction.group(0)\n","      first_char = match[0]\n","      expanded_contraction = contraction_mapping.get(match)\\\n","                              if contraction_mapping.get(match)\\\n","                              else contraction_mapping.get(match.lower())                       \n","      expanded_contraction = first_char+expanded_contraction[1:]\n","      return expanded_contraction\n","      \n","  expanded_text = contractions_pattern.sub(expand_match, text)\n","  # expanded_ text = re.sub(\"'\", \"\", expanded_text)\n","  return expanded_text\n","\n","# remove any character except alphabets\n","def remove_special_characters(text, remove_digits=False):\n","  pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","  text = re.sub(pattern, '', text)\n","  return text\n","\n","# lemmatization\n","def lemmatize_text(text):\n","  text = nlp(text)\n","  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","  return text\n","\n","# remove stopwords \n","def remove_stopwords(text, is_lower_case=False):\n","  tokens = tokenizer.tokenize(text)\n","  tokens = [token.strip() for token in tokens]\n","  if is_lower_case:\n","      filtered_tokens = [token for token in tokens if token not in stopword_list]\n","  else:\n","      filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","  filtered_text = ' '.join(filtered_tokens)    \n","  return filtered_text\n","\n","def normalize_corpus(corpus, contraction_expansion=False,\n","                     accented_char_removal=False, text_lower_case=True, \n","                     text_lemmatization=False, special_char_removal=True, \n","                     stopword_removal=True, remove_digits=True, \n","                     remove_brac_content=True, remove_urls=True):\n","    \n","  normalized_corpus = []\n","  # normalize each document in the corpus\n","  for doc in tqdm(corpus, total=len(corpus)):\n","  # for doc in corpus:\n","    # remove accented characters\n","    if accented_char_removal:\n","      doc = remove_accented_chars(doc)\n","    # expand contractions    \n","    if contraction_expansion:\n","      doc = expand_contractions(doc)\n","    # lowercase the text    \n","    if text_lower_case:\n","      doc = doc.lower()\n","    # remove extra newlines\n","    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n","    # lemmatize text\n","    if text_lemmatization:\n","      doc = lemmatize_text(doc)\n","    if remove_urls:\n","      doc = remove_hyperlinks(doc)\n","    # remove special characters and\\or digits   \n","    if remove_brac_content:\n","      doc = remove_text_within_brackets(doc) \n","    if special_char_removal:\n","      # insert spaces between special characters to isolate them    \n","      special_char_pattern = re.compile(r'([{.(-)!}])')\n","      doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","      doc = remove_special_characters(doc, remove_digits=remove_digits)  \n","    # remove extra whitespace\n","    doc = re.sub(' +', ' ', doc)\n","    # remove stopwords\n","    if stopword_removal:\n","      doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n","        \n","    normalized_corpus.append(doc)\n","      \n","  return normalized_corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1641615371822,"user":{"displayName":"Tirthankar Ghosal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17310949978624243124"},"user_tz":-330},"id":"W-Pn2A1Vf0dJ","outputId":"d3f0ebab-3e8c-4164-ad09-f3d96542787f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14794f27aafb4a158a0b79786948777d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4060 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9bde38598e943faa9ee924f0825c529","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4060 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# replace punctuations \n","main_df['text_a'] = main_df.progress_apply(\n","    lambda row: replace_content(row['text_a']),\n","    axis=1\n",")\n","main_df['text_b'] = main_df.progress_apply(\n","    lambda row: replace_content(row['text_b']),\n","    axis=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BYdXQJHfuCz"},"outputs":[],"source":["# find this in the corpus\n","# For 18 years, Merkel had led the party, opening it up to the left and causing it to take on, in an adapted form, many ideas previously considered the realm of the Social Democrats (SPD). Read more: AfD: What you need to know about Germany\\'s far-right party \\'AKK\\'s achievement\\' In the beginning, AKK tried to harness the conservative wing ‚Äî without success. AKK, as she is also known, said she would also be stepping down as party leader ‚Äî for, as she also announced publicly a couple of hours later, \"in my opinion, the party leader and its candidate for chancellor should be one and the same person.\" On the contrary, it became increasingly clear under AKK that the CDU was deeply divided: One wing sought a return to quintessentially conservative politics, and one, like Angela Merkel, preferred to focus on the socio-liberal center ground. AKK has been criticized from within her own party for showing a lack of leadership It was already clear from the internal party debate that the CDU was at a crossroads. For one,"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":5921,"status":"ok","timestamp":1641615517483,"user":{"displayName":"Tirthankar Ghosal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17310949978624243124"},"user_tz":-330},"id":"Y9GCrwGtqOV3","outputId":"1d920c30-0d2e-4710-c462-1b5f3e375a3b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f354ab218424aca9bf0eedcb69b3edc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4060 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13f560b6c2e2479a8e552f4052a55d38","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4060 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# apply normalization to the corpus\n","main_df['text_a'] = normalize_corpus(main_df['text_a'].to_list())\n","main_df['text_b'] = normalize_corpus(main_df['text_b'].to_list())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXriJFiTt6mj"},"outputs":[],"source":["main_df.to_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/text_rank_data/Longform_textrank_adv/final_data_preproc.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"HYAgIVvOcX_U"},"source":["-----\n","#### <u>FNC-1 Dataset</u>\n","to load fnc-1 dataset, use the one uploaded on huggingface.coü§ó  [link](https://huggingface.co/datasets/nid989/FNC-1)\n","* model name: nid989/FNC-1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFH9aoeWns2u"},"outputs":[],"source":["# APPLY TEXTRANK TO FNC_1 DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmKN9800ZWNn"},"outputs":[],"source":["# data_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/FNC_1/train_bodies.csv')\n","# # rename columns according to the method\n","# data_df.rename(columns={\n","#     \"Body ID\": \"id\",\n","#     \"articleBody\": \"text\",\n","# }, inplace=True, errors=\"raise\")\n","# print(f\"shape: {data_df.shape}\")\n","# # change type to string.\n","# data_df['id'] = data_df['id'].astype('str')\n","# # retrieve sentences\n","# article_sentences = generate_sentences(data_df)\n","# article_sentences = modifiy_gen_sentences(article_sentences)\n","\n","# # apply textrank to article sentences\n","# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/FNC_1\"\n","# processed_dict, faulty_unique_ids = process_batch(article_sentences, word_embeddings, dim=100, file_name=\"fnc_ranked_sentences_data.json\")\n","\n","# # save data\n","# to_json(save_location=path_to_dir, file_name=\"fnc_ranked_sentences_data.json\", dict_=processed_dict) # processed dictionary\n","# to_json(save_location=path_to_dir, file_name=\"fnc_article_sentences_data.json\", dict_=article_sentences) # article sentences\n","\n","# # join and concat ranked data using a topk extraction\n","# ranked_data = [{'idx': key, 'text': \" \".join(extract_top_hyp(value['text'], tokenizer))} for key, value in tqdm(processed_dict.items(), total=len(processed_dict.keys()))]\n","# processed_df = pd.DataFrame(ranked_data, columns=['idx', 'text'])       # processed & ranked data\n","# # rename processed data\n","# processed_df.rename(columns={\n","#     'idx': 'Body ID',\n","#     'text': 'articleBody'\n","# }, inplace=True, errors=\"raise\")\n","# # change dtype of `Body ID` to int64\n","# processed_df['Body ID'] = processed_df['Body ID'].astype(int)\n","# # load train_stances data\n","# stance_df = pd.read_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/FNC_1/train_stances.csv')\n","# # merge both train_stances and processed train_bodies data\n","# fnc_data = pd.merge(stance_df, processed_df, on='Body ID', how='inner')\n","# # apply one-hot encoding to Stance classes\n","# le = preprocessing.LabelEncoder()\n","# le.fit(fnc_data['Stance'])\n","# fnc_data['Stance'] = le.transform(fnc_data['Stance'])\n","# # reorder and shuffle data\n","# fnc_data = fnc_data[['Headline', 'articleBody', 'Stance', 'Body ID']]\n","# fnc_data = fnc_data.sample(frac=1).reset_index(drop=True)\n","\n","# # save to dir.\n","# fnc_data.to_csv(f'{path_to_dir}/fnc_1.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"P78aa-9G5qmo"},"source":["------\n","#### <u> HYPERPARTISAN DATASET </u>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvlOKX6k10GO"},"outputs":[],"source":["# path_to_file = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/sample.csv\"\n","# data_df = pd.read_csv(path_to_file)\n","# # concat preprocessed title and text\n","# data_df['main_text'] = data_df.progress_apply(\n","#     lambda row: row['preproc_title'] + \".\" + row['preproc_text'],\n","#     axis=1\n","# )\n","# # select 15000 samples\n","# data_df = data_df.sample(15000)\n","# # drop unnecessary columns\n","# data_df.drop(['title', 'text', 'preproc_title', 'preproc_text', 'idx'], axis=1, inplace=True)\n","# data_df['id'] = np.arange(data_df.shape[0])\n","# data_df = data_df[['id', 'main_text', 'hyperpartisan', 'bias']]\n","# # rename columns according to the method\n","# data_df.rename(columns={\n","#     \"main_text\": \"text\",\n","# }, inplace=True, errors=\"raise\")\n","\n","# print(f\"shape: {data_df.shape}\")\n","# # change type to string.\n","# data_df['id'] = data_df['id'].astype('str')\n","# # retrieve sentences\n","# article_sentences = generate_sentences(data_df)\n","# article_sentences = modifiy_gen_sentences(article_sentences)\n","\n","# # apply textrank to article sentences\n","# path_to_dir = \"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/\"\n","# processed_dict, faulty_unique_ids = process_batch(article_sentences, word_embeddings, dim=300, fasttext=True, file_name=\"pan_ranked_sentences_data.json\")\n","\n","# # save data\n","# data_df.to_csv(f\"{path_to_dir}sampled_data.csv\", index=False)\n","# to_json(save_location=path_to_dir, file_name=\"pan_ranked_sentences_data.json\", dict_=processed_dict) # processed dictionary\n","# to_json(save_location=path_to_dir, file_name=\"pan_article_sentences_data.json\", dict_=article_sentences) # article sentences\n","\n","# # get saved processed dict\n","# processed_dict = get_json(f\"{path_to_dir}pan_ranked_sentences_data.json\")\n","\n","# sorted_processed_dict = dict()\n","\n","# for key in tqdm(processed_dict.keys(), total=len(processed_dict.keys())):\n","#   text = processed_dict[key]['text']\n","#   scores = processed_dict[key]['score']\n","#   sorted_text_list = sorted(((scores[i],s) for i,s in enumerate(text)), reverse=True)\n","#   ranked_scores = [score for score, _ in sorted_text_list]\n","#   ranked_texts = [text for _, text in sorted_text_list]\n","#   sorted_processed_dict[key] = {\n","#       'score': ranked_scores,\n","#       'text': ranked_texts\n","#   }\n","\n","# # join and concat ranked data using a topk extraction\n","# ranked_data = [{'idx': key, 'text': \" \".join(extract_top_hyp(value['text'], tokenizer))} for key, value in tqdm(sorted_processed_dict.items(), total=len(sorted_processed_dict.keys()))]\n","# processed_df = pd.DataFrame(ranked_data, columns=['idx', 'text'])       # processed & ranked data\n","# # change dtype of `idx` to int64\n","# processed_df['idx'] = processed_df['idx'].astype(int)\n","# # save processed & concatenated data\n","# processed_df.to_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/final_pan_data.csv', index=False)\n","\n","# # extract processed data\n","# pan_data = pd.read_csv(\"/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/final_pan_data.csv\")\n","\n","# data_df_copy = data_df.copy(deep = True)\n","# data_df_copy.drop(['text'], axis=1, inplace=True)\n","# data_df_copy.rename(columns={'id': 'idx'}, inplace=True, errors=\"raise\")\n","# data_df_copy['idx'] = data_df_copy['idx'].astype(int)\n","\n","# merged_pan_data = pd.merge(pan_data, data_df_copy, on='idx', how='inner')\n","# merged_pan_data.to_csv('/content/drive/MyDrive/SemEval-Akash_Nidhir_Rishikesh/SemEval 2022 - Multilingual Document Similarity/Semeval-Task-8/dataset/external_data/hyperpartisan_dataset/Data/final_pan_data.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["1VM-FFtpuTEV","d51GDeleivYq","97gOnbppyWLl","Jf-kmun98caa","9acLzPtUGnUk","HYAgIVvOcX_U","P78aa-9G5qmo"],"name":"Ranking Semeval Task-8 Data.ipynb","provenance":[]},"interpreter":{"hash":"28d20abe702c51411dcaa3905c6de3ac0a98c5080374e33bec0911a89e76b4f2"},"kernelspec":{"display_name":"Python 3.7.1 64-bit ('stanfordnlp': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}
